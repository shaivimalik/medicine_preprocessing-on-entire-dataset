::: {.cell .markdown}
# Introduction

This notebook explores the concept of data leakage, its consequences, and prevention methods. We will investigate a specific type of data leakage: **preprocessing on training and test sets**. To demonstrate the impact of data leakage, we will use the National Health and Nutrition Examination Survey dataset for age prediction, aiming to classify individuals as adults or seniors using a Support Vector Machine (SVM) classifier with rbf kernel. 

We will experiment with two modeling approaches:

- **Oversampling the entire dataset followed by splitting**: This approach may introduce data leakage due to potential information overlap between training and test sets after oversampling.

- **Splitting the dataset before oversampling**: This approach helps mitigate data leakage by ensuring distinct training and test sets.

By comparing the performance of these models, we will examine the impact of data leakage on model accuracy and generalisation.

Overview of the sections:

- [Data Leakage](#data-leakage): We'll explore data leakage and common errors.

- [Retrieve the dataset](#retrieve-the-dataset): We will learn more about the NHANES dataset and retrieve it.

- [SMOTE](#smote): We will implement the SMOTE oversampling technique.

- [Training SVM - with Data Leakage](#training-svm---with-data-leakage): We will oversample the entire dataset before splitting it into training and test sets, and then train the SVM model.

- [Training SVM - without Data Leakage](#training-svm---without-data-leakage): We will split the dataset into training and test sets, then oversample the training set before training the SVM model.

- [Discussion](#discussion): We will evaluate the performance of models and discuss the importance of correct preprocessing.

:::

::: {.cell .markdown}
# Data Leakage

Data leakage occurs when a model learns to recognise patterns or relationships between the features and target variable during training that don't exist in the real-world data. Since these patterns wonâ€™t be present in the real-world data about which the claims are made, models with data leakage errors fail to generalise to unseen data[^1]. Data leakage includes errors such as:

- **No test set:** If the model is trained and tested on the same data, it will perform exceptionally well on the test set, but it will fail on unseen data.

- **Temporal leakage:** This occurs when data from the future is used to train a model that is supposed to predict future events.

- **Duplicates in datasets:** If there are duplicate data points in both the training and test sets, the model can memorize these instances, leading to inflated performance metrics.

- **Pre-processing on training and test set:** If pre-processing is performed on the entire dataset, information about the testing set leaks into the training set. 

- **Model uses features that are not legitimate:** If the model has access to features that should not be legitimately available for use. For example, when information about the target variable is incorporated into the features used for training.

Data leakage leads to overly optimistic estimates of model performance. It is also identified as the major cause behind the reproducibility crisis in ML-Based science[^3].

In this notebook, we will discover the consequences of preprocessing the entire dataset, specifically oversampling, on model performance.

:::

::: {.cell .markdown}
# Retrieve the dataset
The **National Health and Nutrition Examination Survey (NHANES)**, conducted by the Centers for Disease Control and Prevention (CDC), assesses the health and nutritional status of the U.S. In this notebook, we will use a subset of the NHANES database from the **UC Irvine Machine Learning Repository**. 

The dataset contains the following features:

- Respondent's Gender
- If the respondent engages in moderate or vigorous-intensity sports, fitness, or recreational activities in the typical week
- Respondent's Body Mass Index
- Respondent's Blood Glucose after fasting
- If the Respondent is diabetic
- Respondent's Oral
- Respondent's Blood Insulin Levels

The dataset contains information about **2278** respondents, out of which **1914** are categorised as **Adult** and **364** are categorised as **Senior**. Due to the inherent class imbalance, we will utilise the SMOTE oversampling technique to balance the dataset before training a Support Vector Machine with an RBF kernel.

We start by importing the required libraries and loading the dataset using the ucimlrepo package.

:::

::: {.cell .code}
```python
import numpy as np
from ucimlrepo import fetch_ucirepo
from sklearn.svm import SVC
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import accuracy_score, recall_score
from sklearn.model_selection import ShuffleSplit
```
:::

::: {.cell .code}
```python
from ucimlrepo import fetch_ucirepo

# fetch dataset
national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset = fetch_ucirepo(id=887)

# data (as pandas dataframes)
X = national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset.data.features
targets = national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset.data.targets

# metadata
print(national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset.metadata)

# variable information
print(national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset.variables)

y=targets['age_group'].map({'Adult':0.0,'Senior':1.0})
```
:::

::: {.cell .markdown}
# SMOTE

**Synthetic Minority Over-sampling Technique (SMOTE)** is an over-sampling technique in which the minority class is over-sampled by creating synthetic samples[^2]. The synthetic samples are generated with the help of k minority class nearest neighbors. The algorithm generates synthetic samples using the following steps:

- For each minority class sample, perform steps N times for each minority class sample, where N is the desired oversampling ratio:

    - Identify its k-nearest neighbors (typically k=5) within the minority class.

    - Randomly select one of these neighbors.

    - Take the difference between the sample and its random nerest neighbor

    - Multiply the difference by a random number between 0 and 1

    - Add the resultant to the sample to create the synthetic sample

Given below is an implementation of SMOTE using numpy and scikit-learn. We calculate the value of N based on the imbalance present in the input vectors. We generate the synthetic samples with the help of the populate function present inside the sample method.

::: 

::: {.cell .code}
```python
class SMOTE(object):
  """
  This class implements the SMOTE (Synthetic Minority Oversampling Technique) algorithm
  for oversampling imbalanced datasets.
  """
  def __init__(self, n_neighbors=5):

    np.random.seed(15)
    self.k = n_neighbors
    # Initialize the NearestNeighbors object with Euclidean distance
    self.nn = NearestNeighbors(n_neighbors=self.k + 1, metric='euclidean')

  def sample(self, X, y):

    # Check if X and y are numpy arrays
    if not isinstance(X, np.ndarray):
      raise TypeError("X must be a numpy array, got", type(X).__name__,"instead")

    if not isinstance(y, np.ndarray):
      raise TypeError("X must be a numpy array, got", type(X).__name__,"instead")

    def populate(self, N, i, nnarray):

      nonlocal X
      nonlocal synthetic
      nonlocal new_index
      nonlocal parent
      nonlocal minority_X
      while N!=0:
        nn = np.random.randint(1,self.k+1)
        dif = minority_X[nnarray[:,nn]]-X[i]
        gap = np.random.random()
        synthetic[new_index] = X[i] + gap * dif
        parent.append(i)
        new_index += 1
        N -= 1
    
    # Identify unique classes and their counts
    unique_classes, class_counts = np.unique(y, return_counts=True)

    # Determine the minority and majority classes
    minority_class = unique_classes[np.argmin(class_counts)]
    majority_class = unique_classes[np.argmax(class_counts)]

    # Get indices of minority and majority class samples
    majority_idx = np.nonzero(y == majority_class)[0]
    minority_idx = np.nonzero(y == minority_class)[0]

    # Count of samples in each class
    ms = minority_idx.shape[0]
    ml = majority_idx.shape[0]

    print("Samples of minority class:",ms)
    print("Samples of majority class:", ml)

    # Calculate the number of synthetic samples needed
    N = (ml -ms)//ms

    print("Number of synthetic samples to be generated:", N*ms)

    # Initialize the array to hold synthetic samples
    synthetic = np.zeros((N*ms, X.shape[1]))

    new_index = 0

    parent=[i for i in range(X.shape[0])]

    minority_X = X[minority_idx]

    # Fit the nearest neighbors model to the data
    self.nn.fit(minority_X)

    # Generate synthetic samples for each minority class instance
    for i in minority_idx:
      nnarray = self.nn.kneighbors([X[i]], return_distance=False)
      populate(self, N, i, nnarray)

    return np.concatenate([X, synthetic], axis=0), np.concatenate([y, np.ones(N*ms)*minority_class], axis=0), np.array(parent)
```
:::

::: {.cell .markdown}
# Training SVM - with Data Leakage

:::

::: {.cell .code}
```python

```
:::

::: {.cell .markdown}
# Training SVM - without Data Leakage

:::

::: {.cell .code}
```python

```
:::

::: {.cell .markdown}
# Discussion

| Metric        | With Data Leakage | Without Data Leakage|
|:-------------:|:-----------------:|:-------------------:|
| Accuracy      |                   |                     |
| Error         |                   |                     |      
| Specificity   |                   |                     |
| Sensitivity   |                   |                     |

:::

[^1]: Kapoor S, Narayanan A. Leakage and the reproducibility crisis in machine-learning-based science. Patterns (N Y). 2023 Aug 4;4(9):100804. doi: 10.1016/j.patter.2023.100804. PMID: 37720327; PMCID: PMC10499856.

[^2]: Chawla, N.V., Bowyer, K.W., Hall, L.O. and Kegelmeyer, W.P., 2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, pp.321-357.

[^3]: Nisbet, R., Elder, J., and Miner, G. Handbook of Statistical Analysis and Data Mining Applications. Elsevier, 2009. ISBN 978-0-12-374765-5.
