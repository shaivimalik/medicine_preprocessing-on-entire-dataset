::: {.cell .markdown}
# Introduction

In this notebook, we will reproduce the results published in **Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures**. Preterm birth occurs when a baby is born before 37 weeks of pregnancy. It is the leading cause of death in children under the age of five and is responsible for 35% of all newborn deaths. In 2020, an estimated 13.4 million babies were born prematurely, representing more than 1 in 10 births worldwide. The consequences of preterm birth are severe, with approximately 900,000 children dying in 2019 due to related complications. Many survivors face lifelong disabilities, including learning difficulties and visual and hearing problems[^2].

The main challenge in addressing preterm births is the delay in obtaining clinical proof, which can lead to increased labor complications and perinatal mortality[^1]. Early prediction of preterm birth is crucial to prevent these deaths and complications. Once identified, healthcare providers can take essential measures to ensure extra care for the mother during labor and for the newborn baby. Recent advancements in machine learning and deep learning have opened doors for novel approaches to classify between term and preterm deliveries. These powerful tools hold immense promise for improving our ability to identify pregnancies at risk. This notebook aims to reproduce the results of one such study, exploring the use of electrohysterograms signatures to characterize term and preterm deliveries.

## Objectives

- Understand the methodology used in the original paper.
- Implement the described algorithms and techniques.
- Analyze and compare our results with those published in the paper.
- Discuss potential improvements for future research.

***
:::

::: {.cell .markdown}
# Retrieve the data
The **Term-Preterm EHG Database**[^3] is a collection of EHG signals obtained from 1997 to 2005 at the University Medical Centre Ljubljana, Department of Obstetrics and Gynecology. Electrohysterograms Signatures are obtained by placing four electrodes on the abdomen of the mothers. It is a non-invasive method to measure the electrical activity of the uterine muscle. The records were obtained during regular check-ups either around the 22nd week of gestation or around the 32nd week of gestation. 

The TPEGH DB consists of EHG records obtained from 262 women who had full-term pregnancies and 38 whose pregnancies ended prematurely. Each record is composed of three channels, recorded from 4 electrodes:

- the first electrode (E1) was placed 3.5 cm to the left and 3.5 cm above the navel;
- the second electrode (E2) was placed 3.5 cm to the right and 3.5 cm above the navel;
- the third electrode (E3) was placed 3.5 cm to the right and 3.5 cm below the navel;
- the fourth electrode (E4) was placed 3.5 cm to the left and 3.5 cm below the navel.

The differences in the electrical potentials of the electrodes were recorded, producing 3 channels:

- S1 = E2–E1 (first channel);
- S2 = E2–E3 (second channel);
- S3 = E4–E3 (third channel).

Each record consists of two files, a header file (.hea) containing information regarding the record and the data file (.dat) containing signal data.[^4]

We'll begin by acquiring the TPEGH DB (Term-Preterm ElectroHysteroGram Database) and extracting relevant features for our model training. The following cell will:

- Clone the project repository
- Download the TPEGH DB dataset
- Install required dependencies

Note that the download may take some time depending on your internet connection speed.

***
:::

::: {.cell .code}
```python
!git clone https://github.com/shaivimalik/medicine_preprocessing-on-entire-dataset.git
%cd medicine_preprocessing-on-entire-dataset
!pip install -r requirements.txt
!curl -O https://physionet.org/static/published-projects/tpehgdb/term-preterm-ehg-database-1.0.1.zip
!unzip term-preterm-ehg-database-1.0.1.zip
```
:::

::: {.cell .markdown}
# Reproducing the result of "Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures"

**Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures** presents a method for preterm birth prediction using classifiers trained on Electrohysterogram (EHG) signatures. In this study, four features were extracted from EHG signatures: **Median frequency, Shannon energy, Log energy and Lyapunov exponent**. These features were used to distinguish between term and preterm births. 

However, the TPEGH database presents a challenge: class imbalance. It contains a significantly higher number of term birth records (262) compared to preterm births (38). To address the class imbalance issue, the study employed the Adaptive Synthetic Sampling technique. This method generates synthetic samples for the minority class to create a more balanced dataset. 

Using these features, various machine learning models were trained, including different types of Support Vector Machines (Linear, Quadratic, Cubic, Fine Gaussian, Medium Gaussian), Decision Trees, and K-Nearest Neighbor (Medium and Cubic) classifiers. 10-fold cross validation was used to to assess the performance of these models, reporting accuracy, sensitivity, specificity, and error scores for each.

![results from the paper[^1]](../assets/results_from_paper.png)

In this notebook, we will reproduce the model with the highest reported accuracy: the **Support Vector Machine (SVM) with the radial basis function (RBF) kernel (SVM-FG)**. We will train the SVM-FG model and evaluate its performance using the same metrics reported in the paper: accuracy, error, sensitivity, and specificity.  The model achieved the following performance metrics:

| Metric        | Score           |   
|:-------------:|:---------------:|
| Accuracy      | 95.5%           |
| Error         | 4.48            |         
| Specificity   | 97.13           |
| Sensitivity   | 93.51           |

Overview of the sections:

- [Generate Features](#generate-features): We'll discuss the techniques used to extract features from raw EHG signals.

- [Addressing Class Imbalance](#addressing-class-imbalance): We'll discuss the ADASYN oversampling technique and its role in balancing the dataset.

- [SVM Classifier Training and Evaluation](#svm-classifier-training-and-evaluation): We'll implement the process of training the SVM (Support Vector Machine) model with the extracted features and its evaluation using 10-fold cross-validation.

- [Discussion](#discussion): Finally, we'll present and interpret the results obtained from the model.

***
:::

::: {.cell .markdown}
# Generate Features

Here, we will generate features from raw electrohysterogram (EHG) signatures. Raw EHG signals are corrupted due to the presence of noise and artifacts (identifiable disturbances that are not part of the genuine physiological signal). To remove the noise and artifacts and isolate the region of interest, Empirical Mode Decomposition technique was leveraged in the study. EMD decomposes each signal into a series of signal components called Intrinsic Mode Functions (IMFs). IMFs represent different oscillatory modes present in the signal, each with its own amplitude and frequency modulation. The first IMF was identified as the region of interest in the paper. 

We will use Empirical Mode Decomposition to extract Intrinsic Mode Functions. Next, we will compute Median frequency, Shannon energy, Log energy and Lyapunov exponent from IMF-1. These computed features will be used for training our model.

The code cell below automates this process. It creates a directory named `individual_features` to store the feature files for each signal. Then, it executes two Python scripts:
- `all_features.py`: generates the individual feature CSV files for each signal.

- `process_feature_files.py`: combines the individual feature files into a single dataset containing features from all 298 EHG signatures.

Note that 2 EHG signals will be discarded due to their short recording lengths.

***
:::

::: {.cell .code}
```python
!mkdir individual_features
!python3 EHG-Oversampling/experiments/all_features.py term-preterm-ehg-database-1.0.1/tpehgdb individual_features --study FeaturesKhan
!python3 EHG-Oversampling/experiments/process_feature_files.py individual_features ./
```
:::

::: {.cell .markdown}
## Loading the features

In this section, we will load the dataset from the CSV files created in the previous step. 

For a quick refresher on the Pandas module, [check this out](https://pandas.pydata.org/docs/user_guide/10min.html).

:::

::: {.cell .code}
```python
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from smote_variants import ADASYN
from sklearn.svm import SVC
from sklearn.utils import shuffle
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer
```
::: 

::: {.cell .markdown}

Here, we load feature vectors (X) and labels (Y) from CSV files. The `head()` function displays the first few rows of each dataframe for a quick overview.

:::

::: {.cell .code}
```python
#Loading feature vectors
X=pd.read_csv(os.path.join('..','raw_features.csv'))
X.head()
```
:::

::: {.cell .code}
```python
#Loading labels
Y=pd.read_csv(os.path.join('..','target.csv'))
Y.head()
```
:::

::: {.cell .markdown}
## Exploring the dataset

Here, we will identify data types, check for null entries, compute summary statistics, and visualize the dataset using `matplotlib`.

:::

::: {.cell .code}
```python
#Barplot of two classes
labels=['Term','Preterm']
y=np.array(Y['0'])
term_count=np.sum(y)
preterm_count=y.shape[0]-term_count
plt.bar(['Term Births','Preterm Births'],height=[term_count,preterm_count],color=['tab:green','tab:red'])
plt.ylabel("Frequency")
plt.xlabel("Type of Birth")
plt.show()
```
:::

::: {.cell .code}
```python
plt.scatter(X['Rectime'],X['Gestation']-X['Rectime'], c=['tab:green' if y[i]==1.0 else 'tab:red' for i in range(y.shape[0])])
plt.xlabel('Time of Recording (weeks)')
plt.ylabel('Time to birth (weeks)')
plt.show()
```
:::

::: {.cell .markdown}

This cell visualizes the raw EHG signals. The first 1000 samples of each of the three channels of a signal from the TPEGH database are presented.

:::

::: {.cell .code}
```python
#Visualising the EHG signals
import sys
sys.path.append('../EHG-Oversampling')
from ehgfeatures.signal_io import get_signals
ids, signals, all_clin_names, all_clin_values = get_signals('../term-preterm-ehg-database-1.0.1/tpehgdb', n_signals=1)
fig, axs = plt.subplots(3, 1, figsize=(6,10))
axs[0].plot(signals[0][0][:1000])
axs[1].plot(signals[0][1][:1000])
axs[2].plot(signals[0][2][:1000])
plt.show()
```
:::

::: {.cell .code}
```python
#Data of one individual
x_i=X.iloc[100]
categorical_features=['id', 'Gestation', 'Rectime', 'Age', 'Parity', 'Abortions', 'Weight', 'Hypertension_no', 'Hypertension_yes', 'Diabetes_no', 'Diabetes_yes', 'Placental_position_end', 'Placental_position_front', 'Bleeding_first_trimester_no', 'Bleeding_first_trimester_yes', 'Bleeding_second_trimester_no', 'Bleeding_second_trimester_yes', 'Funneling_negative', 'Funneling_positive', 'Smoker_no', 'Smoker_yes']
for i in categorical_features:
    print(i,":", x_i[i])
```
:::

::: {.cell .code}
```python
#Extracting features required for our study
khan_features = [
 	'FeaturesJager_fmed_ch1', 'FeaturesJager_max_lyap_ch1',
 	'FeaturesJager_sampen_ch1', 'FeaturesJager_fmed_ch2',
 	'FeaturesJager_max_lyap_ch2', 'FeaturesJager_sampen_ch2',
 	'FeaturesJager_fmed_ch3', 'FeaturesJager_max_lyap_ch3',
 	'FeaturesJager_sampen_ch3',
 ]

X= X[[c for c in X.columns if c in khan_features or ('FeaturesAcharya' in c and 'SampleEntropy' in c)]]
#Information about the features
X.info()
```
:::

::: {.cell .code}
```python
#Visualisation of the dataset using Principal Component Analysis
from sklearn import decomposition
pca = decomposition.PCA(n_components=2)
pca.fit(X)
X_tran = pca.transform(X)
plt.scatter(X_tran[:,0],X_tran[:,1], c=["tab:red" if y[i]==0.0 else "tab:green"  for i in range(y.shape[0])])
```
:::

::: {.cell .code}
```python
#Statistics of the features
X.describe()
```
:::

::: {.cell .code}
```python
#Checking for null values
X.isnull().sum()
```
:::

::: {.cell .markdown}
# Addressing class imbalance

The TPEHG database exhibits a class imbalance, containing 38 preterm records and 262 term records. Imagine a classifier which always predicts a term birth (probability of preterm birth=0). On the TPEHG database, this classifier would achieve a high accuracy of 87.34% (262/300). However, this high accuracy stems from the imbalanced data, not the model's ability to distinguish between term and preterm births. This classifier would be ineffective at identifying preterm births, our outcome of interest. 

To prevent classifiers from developing biases towards the majority class, sampling techniques are used. These techniques aim to balance the representation of classes present in the dataset. These techniques allow models to learn from a more balanced dataset and improve their generalizability for real-world prediction. 

- Oversampling: The minority class is upsampled to match the count of majority class. 

- Undersampling: The instances majority class are reduced to match the count of the minority class. Undersampling techniques can discard valuable information that could be crucial for model training.

In this notebook, we will use the Adaptive Synthetic Sampling technique to upsample the instances of the minority class (preterm birth).

## Adaptive Synthetic Sampling (ADASYN)

Adaptive Synthetic Sampling is an oversampling technique in which we generate synthetic samples of the minority class to counter the class imbalance present in the original dataset. ADASYN generates more synthetic samples for the minority samples which are _harder to learn_. ADASYN quantifies the learning difficulty of a minority class sample by calculating the proportion of majority class samples among the sample's K nearest neighbors. More majority neighbors imply a harder to learn minority sample.[^5] Let's go through the algorithm in detail:

- **Calculate the degree of class imbalance**:

$$d=m_s/m_l$$

where $m_s$ is the number of minority class examples and $m_l$ is the number of majority class example

- **Calculate the number of synthetic data examples that need to be generated for the minority class:**

$$G=(m_l-m_s)\times\beta$$

where $\beta$ is a parameter used to specify the desired balance level after generation of the synthetic data. $\beta=1$ means a fully balanced data set is created after generalisation process.


- For each example $x_i \in minority class$, **find K nearest neighbors based on the Euclidean distance** in n dimensional space, and **calculate the ratio** $r_i$ defined as: 

$$r_i=\Delta_i/K, $$  $$i= 1, \ldots, m_s$$

where $\Delta_i$ is the number of examples in the K nearest neighbors of $x_i$ that belong to the majority class, therefore $r_i \in [0, 1]$.

- **Normalise $r_i$** according to


$$\hat{r_i}=r_i/\sum_{i=1}^{m_s}r_i$$


- **Calculate the number of synthetic data examples that need to be generated for each minority example $x_i$**:

$$g_i = \hat{r_i} \times G$$

- **For each minority class data example $x_i$, generate $g_i$ synthetic data examples** according to the following steps:
    * Loop from 1 to $g_i$ :
        + Randomly choose one minority example from the K nearest neighbors for data $x_i$
        + Generate the synthetic data example:
    
          $s_i = x_i + (x_{zi} - x_i) \times \lambda$

          where $\lambda$ is a random number: $\lambda \in [0, 1]$


:::

::: {.cell .markdown}
# SVM Classifier Training and Evaluation

In this section, we will train and evaluate the SVM-FG model. The performance of the model will be evaluated using 10-fold cross validation. The following metrics will also be presented:

- Accuracy: It represents the proportion of correct predicts made by the model.

$\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}$

- Error: It represents the proportion of incorrect predictions made by the model.

$\text{Error} = 1 - \text{Accuracy}$

- Specificity: It represents a model's ability to correctly identify negative samples.

$\text{Specificity} = \frac{\text{True Negatives}}{\text{Total Negative Samples}}$

- Sensitivity: It represents a model's ability to correctly identify positive samples.

$\text{Sensitivity} = \frac{\text{True Positives}}{\text{Total Positive Samples}}$

The paper doesn't specify the hyperparameters used to train the model. Therefore, we will use `GridSearchCV` to find optimal hyperparameter values for our classifier. We will then report the performance scores obtained by this optimized classifier.

***
:::

::: {.cell .code}
```python
# Training model

X=np.array(X)

# Oversample the dataset using ADASYN
oversampler = ADASYN(random_state=15)
x_oversamp,y_oversamp = oversampler.sample(X, y)
x_oversamp,y_oversamp = shuffle(x_oversamp, y_oversamp)

# Define specificity and sensitivity scoring functions
def specificity_score(y_true, y_pred):
    return recall_score(y_true, y_pred, pos_label=0)
def sensitivity_score(y_true, y_pred):
    return recall_score(y_true, y_pred)

# Create scorers using make_scorer
specificity = make_scorer(specificity_score)
sensitivity = make_scorer(sensitivity_score)

# Define parameters for grid search
parameters = {'C':[10**i for i in range(-5,5)]}

# Initialize SVM model
svc = SVC(kernel='rbf', random_state=15)

# Define GridSearchCV with custom scorers
scoring = {'accuracy':'accuracy','sensitivity':sensitivity,'specificity':specificity}
clf = GridSearchCV(svc, parameters, cv=10, scoring=scoring, refit='accuracy')

# Perform grid search
clf.fit(x_oversamp, y_oversamp)

# Print results
print("Accuracy:", clf.best_score_)
print("Error:", 1-clf.best_score_)
print("Sensitivity:", clf.cv_results_['mean_test_sensitivity'][clf.best_index_])
print("Specificity:", clf.cv_results_['mean_test_specificity'][clf.best_index_])
print("Best hyperparameter(C):", clf.best_params_)
```
:::

::: {.cell .markdown}
# Discussion

| Metric        | Score           |   
|:-------------:|:---------------:|
| Accuracy      | 94.81%          |
| Error         | 5.19            |         
| Specificity   | 1.0             |
| Sensitivity   | 89.62           |

***
:::

::: {.cell .markdown}
# References

[^1]: M. U. Khan, S. Aziz, S. Ibraheem, A. Butt and H. Shahid, "Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures," 2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Vancouver, BC, Canada, 2019, pp. 0899-0905, doi: 10.1109/IEMCON.2019.893629

[^2]: [WHO, "Preterm birth," 2018.](https://www.who.int/news-room/fact-sheets/detail/preterm-birth)

[^3]: Fele-Žorž, G., Kavšek, G., Novak-Antolič, Ž. et al. A comparison of various linear and non-linear signal processing techniques to separate uterine EMG records of term and pre-term delivery groups. Med Biol Eng Comput 46, 911–922 (2008). https://doi.org/10.1007/s11517-008-0350-y

[^4]: Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.

[^5]: Haibo He, Yang Bai, E. A. Garcia and Shutao Li, "ADASYN: Adaptive synthetic sampling approach for imbalanced learning," 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), Hong Kong, 2008, pp. 1322-1328, doi: 10.1109/IJCNN.2008.4633969. keywords: {Classification algorithms;Decision trees;Algorithm design and analysis;Training data;Machine learning;Accuracy;Machine learning algorithms}

:::
