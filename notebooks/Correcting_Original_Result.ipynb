{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shaivimalik/medicine_preprocessing-on-entire-dataset/blob/main/notebooks/Correcting_Original_Result.ipynb)\n",
    "\n",
    "# “Characterization of Term and Preterm Deliveries using Electrohysterograms Signatures” Without Data Leakage"
   ],
   "id": "006ae102-5f1a-40f9-a39c-e7ef9b688afe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the preceding notebooks, we demonstrated the impact of data leakage\n",
    "on a model’s performance on the test set and real-world data. In this\n",
    "notebook, we will reproduce the results published in **Characterization\n",
    "of Term and Preterm Deliveries using Electrohysterograms Signatures**\n",
    "\\[1\\] without the data leakage error. Our goal here is to demonstrate\n",
    "the correct way of preprocessing the dataset and discuss the changes in\n",
    "the reported metrics upon rectification of the error.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "-   Implement the described techniques and train SVM without data\n",
    "    leakage errors.\n",
    "-   Analyze and compare our results with those published in the paper\n",
    "    and obtained in the previous notebook."
   ],
   "id": "a48ec444-8e40-4e15-ac12-15cd2964fb09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running on Google Colab\n",
    "#!git clone https://github.com/shaivimalik/medicine_preprocessing-on-entire-dataset.git\n",
    "#!pip install -r medicine_preprocessing-on-entire-dataset/requirements.txt\n",
    "#%cd medicine_preprocessing-on-entire-dataset/notebooks"
   ],
   "id": "92dfa79e-dde6-4437-b4be-985acd0f1b12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the data & Generate Features\n",
    "\n",
    "The **Term-Preterm EHG Database** \\[2\\] is a collection of EHG signals\n",
    "obtained from 1997 to 2005 at the University Medical Centre Ljubljana,\n",
    "Department of Obstetrics and Gynecology. The TPEHG DB consists of EHG\n",
    "records obtained from 262 women who had full-term pregnancies and 38\n",
    "whose pregnancies ended prematurely. Each record consists of two files,\n",
    "a header file (.hea) containing information regarding the record and the\n",
    "data file (.dat) containing signal data \\[3\\].\n",
    "\n",
    "We’ll begin by acquiring the TPEHG DB (Term-Preterm ElectroHysteroGram\n",
    "Database) and extracting relevant features for our model training.\n",
    "\n",
    "*Note: The download may take some time depending on your internet\n",
    "connection speed.*"
   ],
   "id": "620c4638-04a9-4917-915b-3cb3852072ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o ../term-preterm-ehg-database-1.0.1.zip https://physionet.org/static/published-projects/tpehgdb/term-preterm-ehg-database-1.0.1.zip\n",
    "!unzip ../term-preterm-ehg-database-1.0.1.zip -d ../"
   ],
   "id": "7970cd29-0f2f-4033-97e5-29cb289e907b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Empirical Mode Decomposition to extract Intrinsic Mode\n",
    "Functions from raw EHG signatures. Then, we will compute Median\n",
    "frequency, Shannon energy, Log energy and Lyapunov exponent from IMF-1.\n",
    "These computed features will be used for training our model.\n",
    "\n",
    "The code cell below automates this process. It creates a directory named\n",
    "`individual_features` to store the feature files for each signal. Then,\n",
    "it executes two Python scripts:\n",
    "\n",
    "-   `all_features.py`: generates the individual feature files for each\n",
    "    signal.\n",
    "\n",
    "-   `process_feature_files.py`: combines the individual feature files\n",
    "    into a single dataset containing features from all 298 EHG\n",
    "    signatures.\n",
    "\n",
    "*Note: 2 EHG signals will be discarded due to their short recording\n",
    "lengths.*"
   ],
   "id": "c2281e41-1f25-480f-b459-62bec2bbc9ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../individual_features\n",
    "!python3 ../EHG-Oversampling/experiments/all_features.py ../term-preterm-ehg-database-1.0.1/tpehgdb ../individual_features --study FeaturesKhan\n",
    "!python3 ../EHG-Oversampling/experiments/process_feature_files.py ../individual_features ../"
   ],
   "id": "e6c1b7ee-9c35-4313-b824-bb7576415c76"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the features\n",
    "\n",
    "In this section, we will load the dataset from the CSV files created in\n",
    "the previous step.\n",
    "\n",
    "We start by importing the required modules."
   ],
   "id": "77b53247-9e13-4ec4-9f10-621b0962ddfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, recall_score, f1_score, recall_score, precision_score, balanced_accuracy_score, make_scorer, auc"
   ],
   "id": "58fe32f9-0c0d-4ab2-82fd-8bbdaa76b449"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load feature matrix (`features`) and labels (`y`) from CSV files. The\n",
    "`head()` function displays the first few rows of each dataframe for a\n",
    "quick overview."
   ],
   "id": "6cc840f2-7f8d-4dd7-a3a9-627d5453d464"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading feature vectors\n",
    "features=pd.read_csv(os.path.join('..','raw_features.csv'))\n",
    "features.head()"
   ],
   "id": "50bc9465-005e-433c-b09f-952326b24fed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading labels\n",
    "y=pd.read_csv(os.path.join('..','target.csv'))\n",
    "y.head()"
   ],
   "id": "9981b317-3104-4e67-896f-7083d77f370b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features required for our study\n",
    "khan_features = [\n",
    "    'FeaturesJager_fmed_ch1', 'FeaturesJager_max_lyap_ch1',\n",
    "    'FeaturesJager_sampen_ch1', 'FeaturesJager_fmed_ch2',\n",
    "    'FeaturesJager_max_lyap_ch2', 'FeaturesJager_sampen_ch2',\n",
    "    'FeaturesJager_fmed_ch3', 'FeaturesJager_max_lyap_ch3',\n",
    "    'FeaturesJager_sampen_ch3',\n",
    " ]\n",
    "generic_features=[ c for c in features.columns if 'FeaturesAcharya' in c and 'SampleEntropy' in c ]\n",
    "\n",
    "# Extract the relevant features for the study\n",
    "X = features[khan_features + generic_features]\n",
    "\n",
    "# Display summary information about the selected features\n",
    "X.info()"
   ],
   "id": "60ed3439-a2a8-4bcf-b49b-dda45b75a1d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier Training and Evaluation without Data Leakage\n",
    "\n",
    "In this section, we will train and evaluate the SVM-FG model without the\n",
    "data leakage error - i.e. with a correct evaluation on new EHG samples,\n",
    "which were not used to oversample training set."
   ],
   "id": "77c5f712-b3b0-46c7-8b43-828dd8fb90a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this in three parts:\n",
    "\n",
    "-   **Oversampling training and test sets separately**: First, we will\n",
    "    do the oversampling inside our KFold CV loop - i.e. oversample\n",
    "    training data and test data separately to get balanced classes in\n",
    "    each set. We will train the model on the training set and evaluate\n",
    "    on test set. We will see that the accuracy due to this procedure is\n",
    "    somewhat less than in the earlier example, where we oversampled\n",
    "    before dividing into training and test.\n",
    "\n",
    "-   **Oversampling training set and evaluating on unprocessed test\n",
    "    set**: The approach above addresses the data leakage due to\n",
    "    oversampling on the entire data set together. However, it is still\n",
    "    not a realistic view of how our model will perform in real usage,\n",
    "    when the prevalence of pre-term birth is much less. In this part, we\n",
    "    will evaluate our model using unprocessed (not oversampled) test\n",
    "    set, to get a better estimate of its real performance.\n",
    "\n",
    "-   **Using pipelines**: When training a machine learning model with\n",
    "    preprocessing steps, it can be helpful to use a\n",
    "    [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "    to cross validate the model fitting and preprocessing together.\n",
    "    Since we use a preprocessing step from the `imbalanced-learn`\n",
    "    package, we use their\n",
    "    [Pipeline](https://imbalanced-learn.org/stable/references/generated/imblearn.pipeline.Pipeline.html)\n",
    "    implementation.\n",
    "\n",
    "*Note: We use 3-fold cross-validation instead of 10-fold to ensure\n",
    "sufficient minority samples in each fold.*"
   ],
   "id": "4cf5dca2-895d-436b-ba55-0af2b4f21b70"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling training and test sets separately\n",
    "\n",
    "In this approach, we create a `StratifiedKFold` instance to perform\n",
    "3-fold cross validation. We then define a hyperparameter grid for grid\n",
    "search and initialize numpy arrays to store metrics obtained for each\n",
    "fold. The process for each fold is as follows:\n",
    "\n",
    "-   Oversample the training set using ADASYN \\[4\\] to address class\n",
    "    imbalance.\n",
    "-   Use grid search to find optimal hyperparameter values.\n",
    "-   Plot validation accuracy for various combinations of gamma and C\n",
    "    parameters obtained during grid search.\n",
    "-   Oversample the test set.\n",
    "-   Evaluate the optimized classifier on the oversampled test set.\n",
    "-   Store the resulting metrics in the corresponding arrays.\n",
    "\n",
    "Finally, we report the mean values for test accuracy, error,\n",
    "sensitivity, specificity, precision, f1-score and negative predictive\n",
    "value across all folds. We also present ROC curves for each fold.\n",
    "\n",
    "*Note: `StratifiedKFold` is used to obtain balanced folds, ensuring\n",
    "equal representation of minority samples across folds*"
   ],
   "id": "af3f594b-d1da-4d17-9033-5939000bcbd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "gamma_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "C_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "\n",
    "# Dictionary to store performance metrics\n",
    "metrics_acc = {'test_accuracy': np.zeros(kfold.get_n_splits()), 'test_error': np.zeros(kfold.get_n_splits()), 'test_balanced_accuracy': np.zeros(kfold.get_n_splits()), \n",
    "                'test_specificity': np.zeros(kfold.get_n_splits()), 'test_sensitivity': np.zeros(kfold.get_n_splits()), 'test_precision': np.zeros(kfold.get_n_splits()), \n",
    "                'test_negative_predictive_value': np.zeros(kfold.get_n_splits()), 'test_f1_score (preterm birth)': np.zeros(kfold.get_n_splits())}\n",
    "\n",
    "# Create figure to plot heatmaps\n",
    "fig_heatmap, axes = plt.subplots(nrows=1, ncols=kfold.get_n_splits(), figsize=(36, 6))\n",
    "\n",
    "# Initialize ADASYN oversampler\n",
    "oversampler = imblearn.over_sampling.ADASYN(n_neighbors=5, random_state=15)\n",
    "\n",
    "# Create figure to plot ROC curve\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Loop through the folds of the cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X.to_numpy()[train_index], X.to_numpy()[test_index]\n",
    "    y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
    "\n",
    "    print(\"Grid Search Fold:\",fold+1)\n",
    "\n",
    "    # Initialize array to store mean val scores\n",
    "    mean_val_score = np.zeros((C_range.shape[0], gamma_range.shape[0]))\n",
    "\n",
    "    # Perform nested cross-validation for hyperparameter tuning\n",
    "    for idx, (train_index_opt, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "        # Split training data\n",
    "        X_train_opt, y_train_opt = X_train[train_index_opt], y_train[train_index_opt]\n",
    "        X_val_opt, y_val_opt = X_train[val_index], y_train[val_index]\n",
    "\n",
    "        # Apply oversampling to training and validation sets\n",
    "        X_train_opt_oversampled, y_train_opt_oversampled = oversampler.fit_resample(X_train_opt, y_train_opt)\n",
    "        X_val_opt_oversampled, y_val_opt_oversampled = oversampler.fit_resample(X_val_opt, y_val_opt)\n",
    "\n",
    "        # Grid search over C and gamma parameters\n",
    "        for i in range(C_range.shape[0]):\n",
    "            for j in range(gamma_range.shape[0]):\n",
    "                svc_opt = SVC(kernel='rbf', C=C_range[i], gamma=gamma_range[j], random_state=15)\n",
    "                svc_opt.fit(X_train_opt_oversampled, y_train_opt_oversampled)\n",
    "                y_pred_opt = svc_opt.predict(X_val_opt_oversampled)\n",
    "                mean_val_score[i,j] += accuracy_score(y_val_opt_oversampled, y_pred_opt)\n",
    "\n",
    "    # Calculate mean test score across all inner folds\n",
    "    mean_val_score = mean_val_score/kfold.get_n_splits()\n",
    "    \n",
    "    # Find best hyperparameters\n",
    "    C_index, gamma_index = np.unravel_index(np.argmax(mean_val_score, axis=None), mean_val_score.shape)\n",
    "    print(\"C:\",C_range[C_index])\n",
    "    print(\"gamma:\", gamma_range[gamma_index])\n",
    "    print(\"Validation accuracy:\", mean_val_score[C_index, gamma_index])\n",
    "\n",
    "    # Plot heatmap\n",
    "    im = axes[fold].imshow(mean_val_score, interpolation=\"nearest\", cmap='Blues', vmin=0.0, vmax=1.0)\n",
    "    axes[fold].set_ylabel(\"C\")\n",
    "    axes[fold].set_xlabel(\"gamma\")\n",
    "    axes[fold].set_xticks(np.arange(gamma_range.shape[0]), labels=gamma_range, rotation=45)\n",
    "    axes[fold].set_yticks(np.arange(C_range.shape[0]), labels=C_range)\n",
    "    axes[fold].set_title(f\"Validation accuracy fold {fold + 1}\")\n",
    "\n",
    "    # Oversample train & test set\n",
    "    X_test_oversampled, y_test_oversampled = oversampler.fit_resample(X_test, y_test)\n",
    "    X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train model with optimal hyperparameters\n",
    "    svm = SVC(kernel='rbf', C=C_range[C_index], gamma=gamma_range[gamma_index], random_state=15)\n",
    "    svm.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_oversampled = svm.predict(X_test_oversampled)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_acc['test_accuracy'][fold] = accuracy_score(y_test_oversampled, y_pred_oversampled)\n",
    "    metrics_acc['test_error'][fold] = 1-accuracy_score(y_test_oversampled, y_pred_oversampled)\n",
    "    metrics_acc['test_sensitivity'][fold] = recall_score(y_test_oversampled, y_pred_oversampled)\n",
    "    metrics_acc['test_specificity'][fold] = recall_score(y_test_oversampled, y_pred_oversampled, pos_label=0)\n",
    "    metrics_acc['test_precision'][fold] = precision_score(y_test_oversampled, y_pred_oversampled)\n",
    "    metrics_acc['test_negative_predictive_value'][fold] = precision_score(y_test_oversampled, y_pred_oversampled, pos_label=0)\n",
    "    metrics_acc['test_balanced_accuracy'][fold] = balanced_accuracy_score(y_test_oversampled, y_pred_oversampled)\n",
    "    metrics_acc['test_f1_score (preterm birth)'][fold] = f1_score(y_test_oversampled, y_pred_oversampled, pos_label=0)\n",
    "\n",
    "    # Plot ROC \n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        svm,\n",
    "        X_test_oversampled,\n",
    "        y_test_oversampled,\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(fold == kfold.get_n_splits() - 1),\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "# Add colorbar to heatmap\n",
    "plt.colorbar(im)\n",
    "# Display the heatmaps\n",
    "fig_heatmap.show()\n",
    "\n",
    "# Display ROC\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label 'term-birth')\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame from the performance metrics\n",
    "metrics_df = pd.DataFrame(metrics_acc)\n",
    "\n",
    "# Average performance on the test set\n",
    "print(\"Average performance on test set:\")\n",
    "print(metrics_df.mean())\n",
    "# Standard error of the performance metrics\n",
    "print(\"Standard error:\")\n",
    "print(metrics_df.std() / np.sqrt(kfold.get_n_splits()))"
   ],
   "id": "ff1f4020-d24f-42eb-907c-dce574f150e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scenario with data leakage, we achieved an accuracy of 97% on a\n",
    "balanced test set but we can see that this was an overly optimistic\n",
    "evaluation because of data leakage. When we correct the data leakage by\n",
    "oversampling training, test and validation sets separately, we are only\n",
    "able to achieve 57% accuracy."
   ],
   "id": "838f4ef7-23f5-4554-be14-651337ebc10d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling training set and evaluating on unprocessed test set\n",
    "\n",
    "This approach follows a similar methodology to the previous one, with a\n",
    "modification: **we omit oversampling the test set**. Evaluating on a\n",
    "balanced test set does not reflect true performance, as the model will\n",
    "be used in the real world, where the prevalence of preterm birth is less\n",
    "than 50%. By preserving the original distribution of the test set, we\n",
    "maintain its representation of real-world data, thus providing a more\n",
    "accurate evaluation of the model’s practical performance.\n",
    "\n",
    "*Note: Baseline model which predicts term birth for all samples would\n",
    "achieve an accuracy of 87% on the TPEHG DB.*"
   ],
   "id": "a424dcab-4782-4374-b0ef-c662ddd91bcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "gamma_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "C_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "\n",
    "# Dictionary to store performance metrics\n",
    "metrics_acc_cor = {'test_accuracy': np.zeros(kfold.get_n_splits()), 'test_error': np.zeros(kfold.get_n_splits()), 'test_balanced_accuracy': np.zeros(kfold.get_n_splits()), \n",
    "                   'test_specificity': np.zeros(kfold.get_n_splits()), 'test_sensitivity': np.zeros(kfold.get_n_splits()), 'test_precision': np.zeros(kfold.get_n_splits()), \n",
    "                   'test_negative_predictive_value': np.zeros(kfold.get_n_splits()), 'test_f1_score (preterm birth)': np.zeros(kfold.get_n_splits())}\n",
    "\n",
    "# Create figure to plot heatmaps\n",
    "fig_heatmap, axes = plt.subplots(nrows=1, ncols=kfold.get_n_splits(), figsize=(36, 6))\n",
    "\n",
    "# Initialize ADASYN oversampler\n",
    "oversampler = imblearn.over_sampling.ADASYN(n_neighbors=5, random_state=15)\n",
    "\n",
    "# Loop through the folds of the cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X.to_numpy()[train_index], X.to_numpy()[test_index]\n",
    "    y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
    "\n",
    "    print(\"Grid Search Fold:\",fold+1)\n",
    "\n",
    "    # Initialize array to store mean val scores\n",
    "    mean_val_score = np.zeros((C_range.shape[0], gamma_range.shape[0]))\n",
    "\n",
    "    # Perform nested cross-validation for hyperparameter tuning\n",
    "    for idx, (train_index_opt, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "        # Split training data\n",
    "        X_train_opt, y_train_opt = X_train[train_index_opt], y_train[train_index_opt]\n",
    "        X_val_opt, y_val_opt = X_train[val_index], y_train[val_index]\n",
    "\n",
    "        # Apply oversampling to training set\n",
    "        X_train_opt_oversampled, y_train_opt_oversampled = oversampler.fit_resample(X_train_opt, y_train_opt)\n",
    "\n",
    "        # Grid search over C and gamma parameters\n",
    "        for i in range(C_range.shape[0]):\n",
    "            for j in range(gamma_range.shape[0]):\n",
    "                svc_opt = SVC(kernel='rbf', C=C_range[i], gamma=gamma_range[j], random_state=15)\n",
    "                svc_opt.fit(X_train_opt_oversampled, y_train_opt_oversampled)\n",
    "                y_pred_opt = svc_opt.predict(X_val_opt)\n",
    "                mean_val_score[i,j] += accuracy_score(y_val_opt, y_pred_opt)\n",
    "\n",
    "    # Calculate mean test score across all inner folds\n",
    "    mean_val_score = mean_val_score/kfold.get_n_splits()\n",
    "    \n",
    "    # Find best hyperparameters\n",
    "    C_index, gamma_index = np.unravel_index(np.argmax(mean_val_score, axis=None), mean_val_score.shape)\n",
    "    print(\"C:\",C_range[C_index])\n",
    "    print(\"gamma:\", gamma_range[gamma_index])\n",
    "    print(\"Validation accuracy:\", mean_val_score[C_index, gamma_index])\n",
    "\n",
    "    # Plot heatmap\n",
    "    im = axes[fold].imshow(mean_val_score, interpolation=\"nearest\", cmap='Blues', vmin=0.0, vmax=1.0)\n",
    "    axes[fold].set_ylabel(\"C\")\n",
    "    axes[fold].set_xlabel(\"gamma\")\n",
    "    axes[fold].set_xticks(np.arange(gamma_range.shape[0]), labels=gamma_range, rotation=45)\n",
    "    axes[fold].set_yticks(np.arange(C_range.shape[0]), labels=C_range)\n",
    "    axes[fold].set_title(f\"Validation accuracy fold {fold + 1}\")\n",
    "\n",
    "    # Oversample train set\n",
    "    X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train model with optimal hyperparameters\n",
    "    svm = SVC(kernel='rbf', C=C_range[C_index], gamma=gamma_range[gamma_index], random_state=15)\n",
    "    svm.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_acc_cor['test_accuracy'][fold] = accuracy_score(y_test, y_pred)\n",
    "    metrics_acc_cor['test_error'][fold] = 1-accuracy_score(y_test, y_pred)\n",
    "    metrics_acc_cor['test_sensitivity'][fold] = recall_score(y_test, y_pred)\n",
    "    metrics_acc_cor['test_specificity'][fold] = recall_score(y_test, y_pred, pos_label=0)\n",
    "    metrics_acc_cor['test_precision'][fold] = precision_score(y_test, y_pred)\n",
    "    metrics_acc_cor['test_negative_predictive_value'][fold] = precision_score(y_test, y_pred, pos_label=0, zero_division=0.0)\n",
    "    metrics_acc_cor['test_balanced_accuracy'][fold] = balanced_accuracy_score(y_test, y_pred)\n",
    "    metrics_acc_cor['test_f1_score (preterm birth)'][fold] = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "# Add colorbar to heatmap\n",
    "plt.colorbar(im)\n",
    "# Display the heatmaps\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame from the performance metrics\n",
    "metrics_df_cor = pd.DataFrame(metrics_acc_cor)\n",
    "\n",
    "# Average performance on the test set\n",
    "print(\"Average performance on test set:\")\n",
    "print(metrics_df_cor.mean())\n",
    "# Standard error of the performance metrics\n",
    "print(\"Standard error:\")\n",
    "print(metrics_df_cor.std() / np.sqrt(kfold.get_n_splits()))"
   ],
   "id": "0a5fbfe3-f23c-4d63-a675-0549a4435c21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipelines\n",
    "\n",
    "In our final approach, we demonstrate that the above implementation can\n",
    "be achieved using `Pipeline` and `GridSearchCV`."
   ],
   "id": "cde5d178-9dbb-47ca-ae1f-65813b98659f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = {\"accuracy\":\"accuracy\", \"balanced_accuracy\": \"balanced_accuracy\", \"specificity\": make_scorer(recall_score, pos_label=0), \n",
    "                \"sensitivity\": \"recall\", \"precision\": \"precision\", \"negative_predictive_value\": make_scorer(precision_score, pos_label=0, zero_division=0.0), \n",
    "                \"f1_score (preterm birth)\": make_scorer(f1_score, pos_label=0)}"
   ],
   "id": "e6e74b7c-bc10-4cf3-ac8a-c3b8372ab877"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "C_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "param_grid = {'SVM__C': C_range, 'SVM__gamma': gamma_range}\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = imblearn.pipeline.Pipeline([\n",
    "        ('ADASYN', imblearn.over_sampling.ADASYN(random_state=5)),\n",
    "        ('SVM', SVC(kernel='rbf', random_state=5))\n",
    "    ])\n",
    "\n",
    "# Define number of splits for StratifiedKFold\n",
    "K=3\n",
    "\n",
    "# Define GridSearchCV\n",
    "clf = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15), scoring='accuracy')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_acc = cross_validate(clf, X.to_numpy(), y.to_numpy(), scoring=eval_metrics, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15))\n",
    "\n",
    "# Create a DataFrame from the performance metrics\n",
    "cv_acc_df = pd.DataFrame(cv_results_acc)\n",
    "\n",
    "# Average performance on the test set\n",
    "print(\"Average performance on test set:\")\n",
    "print(cv_acc_df.mean())\n",
    "# Standard error of the performance metrics\n",
    "print(\"Standard error:\")\n",
    "print(cv_acc_df.std() / np.sqrt(K))"
   ],
   "id": "61e6a7f0-346a-4f59-842b-2e07f41f710d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model basically learns to predict term births for all samples. In\n",
    "this scenario with an imbalanced dataset, we might not want to optimise\n",
    "accuracy because we might care more about identifying women who are at\n",
    "risk of preterm birth.\n",
    "\n",
    "Let’s optimize for `balanced_accuracy` using `Pipeline` and\n",
    "`GridSearchCV`."
   ],
   "id": "a65e94bf-056b-4523-b35c-163f3927db8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "C_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "param_grid = {'SVM__C': C_range, 'SVM__gamma': gamma_range}\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = imblearn.pipeline.Pipeline([\n",
    "        ('ADASYN', imblearn.over_sampling.ADASYN(random_state=5)),\n",
    "        ('SVM', SVC(kernel='rbf', random_state=5))\n",
    "    ])\n",
    "\n",
    "# Define number of splits for StratifiedKFold\n",
    "K=3\n",
    "\n",
    "# Define GridSearchCV\n",
    "clf = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15), scoring='balanced_accuracy')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_balanced_acc = cross_validate(clf, X.to_numpy(), y.to_numpy(), scoring=eval_metrics, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15))\n",
    "\n",
    "# Create a DataFrame from the performance metrics\n",
    "cv_bal_acc_df = pd.DataFrame(cv_results_balanced_acc)\n",
    "\n",
    "# Average performance on the test set\n",
    "print(\"Average performance on test set:\")\n",
    "print(cv_bal_acc_df.mean())\n",
    "# Standard error of the performance metrics\n",
    "print(\"Standard error:\")\n",
    "print(cv_bal_acc_df.std() / np.sqrt(K))"
   ],
   "id": "ae5f3d4c-489d-460f-a711-6454f2423d1c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s optimize for specificity and see how it affects our results."
   ],
   "id": "77be2f51-05aa-4b14-b758-c9a2ee49f671"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "C_range = np.logspace(start=-5, stop=5, num=11, base=10)\n",
    "param_grid = {'SVM__C': C_range, 'SVM__gamma': gamma_range}\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = imblearn.pipeline.Pipeline([\n",
    "        ('ADASYN', imblearn.over_sampling.ADASYN(random_state=5)),\n",
    "        ('SVM', SVC(kernel='rbf', random_state=5))\n",
    "    ])\n",
    "\n",
    "# Define number of splits for StratifiedKFold\n",
    "K=3\n",
    "\n",
    "# Define GridSearchCV\n",
    "clf = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15), scoring=make_scorer(recall_score, pos_label=0))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_specificity = cross_validate(clf, X.to_numpy(), y.to_numpy(), scoring=eval_metrics, cv=StratifiedKFold(n_splits=K, shuffle=True, random_state=15))\n",
    "\n",
    "# Create a DataFrame from the performance metrics\n",
    "cv_specificity_df = pd.DataFrame(cv_results_specificity)\n",
    "\n",
    "# Average performance on the test set\n",
    "print(\"Average performance on test set:\")\n",
    "print(cv_specificity_df.mean())\n",
    "# Standard error of the performance metrics\n",
    "print(\"Standard error:\")\n",
    "print(cv_specificity_df.std() / np.sqrt(K))"
   ],
   "id": "14422b0e-caf3-46fa-b496-bbea81aab27f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we create a barplot to visualise our results."
   ],
   "id": "5930f309-beca-4bd3-bcc7-a010a6d14529"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [metrics_acc, cv_results_acc, cv_results_balanced_acc, cv_results_specificity]\n",
    "labels = ['Grid Search Metric - Accuracy (Oversampled Test Set)', 'Grid Search Metric - Accuracy (Unprocessed Test Set)', \n",
    "              'Grid Search Metric - Balanced Accuracy (Unprocessed Test Set)', 'Grid Search Metric - Specificity (Unprocessed Test Set)'] \n",
    "\n",
    "# Get the keys \n",
    "keys = ['test_accuracy', 'test_balanced_accuracy', 'test_sensitivity', 'test_precision', 'test_specificity', 'test_f1_score (preterm birth)', 'test_negative_predictive_value']\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Set width of each bar and positions of the bars\n",
    "bar_width = 0.2\n",
    "r = np.arange(len(keys))\n",
    "\n",
    "# Plot bars\n",
    "for i, d in enumerate(results):\n",
    "    values = [np.mean(d[key]) for key in keys]\n",
    "    ax.bar(r + i*bar_width, values, width=bar_width, label=labels[i])\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metric Values')\n",
    "ax.set_xticks(r + bar_width * 1.5)\n",
    "ax.set_xticklabels(keys, rotation=45, ha='right')\n",
    "ax.legend(loc='upper right', fontsize = 7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2d587865-f581-4305-8f51-f59555637713"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "| Metric | Original | Reproduced With Data Leakage | Reproduced Without Data Leakage (Oversampled test set) | Reproduced Without Data Leakage (Unprocessed test set) |\n",
    "|:----------:|:-------:|:---------------------:|:---------------:|:-----------:|\n",
    "| Accuracy | 95.5 | 97.12 ± 0.74 | 57.63 ± 3.56 | 87.25 ± 0.32 |\n",
    "| Error | 4.48 | 2.88 ± 0.74 | 42.37 ± 3.56 | 12.75 ± 0.32 |\n",
    "| Specificity | 97.13 | 99.62 ± 0.38 | 35.00 ± 13.92 | 0.00 ± 0.00 |\n",
    "| Sensitivity | 93.51 | 94.62 ± 1.41 | 79.68 ± 9.31 | 100.00 ± 0.00 |\n",
    "\n",
    "By comparing the results from ‘Reproduced With Data Leakage’ and\n",
    "‘Reproduced Without Data Leakage (Oversampled test set)’, we can verify\n",
    "that data leakage led to overly optimistic estimates of model\n",
    "performance. While we oversampled the test set in the latter scenario to\n",
    "provide a fair comparison, this is not recommended in practice for the\n",
    "following reasons:\n",
    "\n",
    "-   It changes the distribution of the test data.\n",
    "-   The oversampled test set no longer represents real-world data.\n",
    "\n",
    "To provide a realistic view of the model’s performance, we have also\n",
    "reported metrics obtained using correct preprocessing and evaluation\n",
    "procedures without modifying the test set. These results reflect the\n",
    "model’s performance on unseen data.\n",
    "\n",
    "| Metric | Reproduced Without Data Leakage (Unprocessed test set) GridSearchCV scoring=“accuracy” | Reproduced Without Data Leakage (Unprocessed test set) GridSearchCV scoring=“balanced_accuracy” | Reproduced Without Data Leakage (Unprocessed test set) GridSearchCV scoring=make_scorer(recall_score, pos_label=0) (Specificity) |\n",
    "|:-------------------------:|:-------------:|:-------------:|:--------------:|\n",
    "| Accuracy | 87.25 ± 0.15 | 66.5 ± 4.88 | 27.5 ± 3.47 |\n",
    "| Balanced Accuracy | 50.0 ± 0.0 | 48.25 ± 0.15 | 51.57 ± 0.5 |\n",
    "| Specificity | 0.0 ± 0.0 | 23.72 ± 6.29 | 83.97 ± 3.79 |\n",
    "| Sensitivity | 100.0 ± 0.0 | 72.79 ± 6.58 | 19.16 ± 4.61 |\n",
    "| Precision | 87.25 ± 0.15 | 86.64 ± 0.18 | 59.44 ± 14.01 |\n",
    "| Negative Predictive Value | 0.0 ± 0.0 | 7.57 ± 1.79 | 13.23 ± 0.03 |\n",
    "| F1-score (preterm birth) | 0.0 ± 0.0 | 11.35 ± 2.73 | 22.81 ± 0.1 |\n",
    "\n",
    "Summary:\n",
    "\n",
    "-   With data leakage, we achieved high accuracy and were also able to\n",
    "    detect preterm birth (high specificity) but this was not a true\n",
    "    result.\n",
    "-   After correcting the data leakage, we achieve much lower accuracy\n",
    "    (57.63%) and detect few preterm births (specificity=35%) on a\n",
    "    balanced test set.\n",
    "-   When using a test set with a realistic preterm to term birth ratio:\n",
    "    -   if we optimise for accuracy, the model learns to predict term\n",
    "        birth for all samples (accuracy = 87%, specificity=0%) which is\n",
    "        not useful.\n",
    "    -   if we optimise for balanced accuracy, the model is able to\n",
    "        predict some of the preterm birth (specificity=23.72%,\n",
    "        accuracy=66.5%).\n",
    "    -   if we optimise for specificity, the model can predict more\n",
    "        preterm birth although with lower accuracy overall (accuracy\n",
    "        =27.5%, specificity=83.97%).\n",
    "\n",
    "Fun Experiments:\n",
    "\n",
    "-   **Classifier Exploration**: Test out Logistic Regression, Decision\n",
    "    Trees, and Random Forests to see how each performs on our data.\n",
    "\n",
    "-   **Oversampling Techniques**: Try out SMOTE and RandomOverSampler to\n",
    "    address the class imbalance."
   ],
   "id": "a059cc07-17c2-4c3c-b6e2-aad4afec8891"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\]: M. U. Khan, S. Aziz, S. Ibraheem, A. Butt and H. Shahid,\n",
    "“Characterization of Term and Preterm Deliveries using\n",
    "Electrohysterograms Signatures,” 2019 IEEE 10th Annual Information\n",
    "Technology, Electronics and Mobile Communication Conference (IEMCON),\n",
    "Vancouver, BC, Canada, 2019, pp. 0899-0905, doi:\n",
    "10.1109/IEMCON.2019.893629\n",
    "\n",
    "\\[2\\]: Fele-Žorž, G., Kavšek, G., Novak-Antolič, Ž. et al. A comparison\n",
    "of various linear and non-linear signal processing techniques to\n",
    "separate uterine EMG records of term and pre-term delivery groups. Med\n",
    "Biol Eng Comput 46, 911–922 (2008).\n",
    "https://doi.org/10.1007/s11517-008-0350-y\n",
    "\n",
    "\\[3\\]: Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P.\n",
    "C., Mark, R., … & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and\n",
    "PhysioNet: Components of a new research resource for complex physiologic\n",
    "signals. Circulation \\[Online\\]. 101 (23), pp. e215–e220.\n",
    "\n",
    "\\[4\\]: Haibo He, Yang Bai, E. A. Garcia and Shutao Li, “ADASYN: Adaptive\n",
    "synthetic sampling approach for imbalanced learning,” 2008 IEEE\n",
    "International Joint Conference on Neural Networks (IEEE World Congress\n",
    "on Computational Intelligence), Hong Kong, 2008, pp. 1322-1328, doi:\n",
    "10.1109/IJCNN.2008.4633969. keywords: {Classification\n",
    "algorithms;Decision trees;Algorithm design and analysis;Training\n",
    "data;Machine learning;Accuracy;Machine learning algorithms}"
   ],
   "id": "e22f6779-1294-40aa-95f1-9b5e8438a879"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
